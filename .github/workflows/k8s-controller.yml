name: Kubernetes Setup

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  # Переменные для containerd (соответствуют улучшенному containerd.go)
  CONTAINERD_MAX_RETRIES: 150
  KUBECONFIG: ${{ github.workspace }}/.kube/config

jobs:
  setup-k8s:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Увеличиваем для медленных runner'ов
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.22'
        cache: true

    - name: Install dependencies
      run: |
        go mod download
        go mod verify

    # Новый шаг: подготавливаем среду для containerd
    - name: Prepare containerd environment
      run: |
        echo "=== Preparing containerd environment ==="
        
        # Создаем необходимые директории с правильными правами
        sudo mkdir -p /var/lib/containerd /run/containerd /var/log/kubernetes /tmp/containerd
        sudo mkdir -p /var/lib/kubernetes/pki
        
        # Устанавливаем правильные права для containerd директорий
        sudo chown -R root:root /var/lib/containerd
        sudo chown -R root:root /run/containerd
        sudo chmod -R 755 /var/lib/containerd
        sudo chmod -R 755 /run/containerd
        
        # Права для логов - чтобы мог читать текущий пользователь
        sudo chown -R root:$USER /var/log/kubernetes
        sudo chmod -R 775 /var/log/kubernetes
        
        # Очищаем старые файлы если есть
        sudo rm -f /run/containerd/containerd.sock*
        sudo rm -f /var/lib/containerd/io.containerd.metadata.v1.bolt/meta.db.lock
        
        # Проверяем доступное место на диске
        echo ""
        echo "=== System resources ==="
        df -h /var/lib
        free -h
        
        # Убиваем старые процессы containerd если есть
        sudo pkill -f containerd || true
        sudo pkill -f dockerd || true  # Убиваем docker если запущен
        sleep 3
        
        # Проверяем что директории созданы правильно
        echo ""
        echo "=== Directory permissions ==="
        ls -la /run/ | grep containerd || echo "containerd dir not found"
        ls -la /var/lib/ | grep containerd || echo "containerd lib dir not found"
        
        echo "✓ Environment prepared"

    - name: Build binary
      run: |
        echo "=== Building installer ==="
        go build -v -o k8s-installer ./cmd/installer
        chmod +x k8s-installer
        echo "✓ Binary built successfully"

    # Улучшенный запуск installer'а с дополнительными опциями
    - name: Run installer with containerd optimizations
      run: |
        echo "=== Running installer ==="
        
        # Экспортируем переменные для нашего улучшенного containerd.go
        export CONTAINERD_MAX_RETRIES=150
        export TMPDIR=/tmp
        
        # Запускаем с дополнительным логированием
        sudo -E ./k8s-installer -skip-api-wait -verbose 2>&1 | tee installer.log
        
        echo ""
        echo "=== Installation completed ==="
        echo "Exit code: $?"

    # Более детальная проверка containerd после установки
    - name: Verify containerd installation
      run: |
        echo "=== Verifying containerd ==="
        
        # Ждем немного для стабилизации
        sleep 5
        
        # Проверяем процесс
        echo "=== Process check ==="
        if pgrep -f containerd > /dev/null; then
          echo "✓ Containerd process is running"
          ps aux | grep containerd | grep -v grep
        else
          echo "✗ Containerd process not found"
          exit 1
        fi
        
        echo ""
        echo "=== Directory permissions check ==="
        ls -la /run/containerd/ || echo "Cannot access /run/containerd/"
        ls -la /var/lib/containerd/ || echo "Cannot access /var/lib/containerd/"
        
        # Проверяем права доступа
        if [ ! -r "/run/containerd" ]; then
          echo "⚠ Fixing /run/containerd permissions"
          sudo chmod 755 /run/containerd
        fi
        
        echo ""
        echo "=== Socket check (with retry) ==="
        SOCKET_FOUND=false
        for i in {1..30}; do
          if [ -S "/run/containerd/containerd.sock" ]; then
            echo "✓ Socket exists (attempt $i)"
            ls -la /run/containerd/
            SOCKET_FOUND=true
            break
          else
            echo "⚠ Socket not found, waiting... (attempt $i/30)"
            sleep 2
          fi
        done
        
        if [ "$SOCKET_FOUND" = false ]; then
          echo "✗ Socket not created after 60 seconds"
          echo ""
          echo "=== Containerd logs ==="
          sudo tail -30 /var/log/kubernetes/containerd.log || echo "No logs found"
          exit 1
        fi
        
        echo ""
        echo "=== CRI test with retries ==="
        export PATH=$PATH:./kubebuilder/bin
        
        CRI_READY=false
        for i in {1..20}; do
          if timeout 10 crictl --runtime-endpoint unix:///run/containerd/containerd.sock version >/dev/null 2>&1; then
            echo "✓ CRI is responsive (attempt $i)"
            CRI_READY=true
            break
          else
            echo "⚠ CRI not ready, waiting... (attempt $i/20)"
            sleep 3
          fi
        done
        
        if [ "$CRI_READY" = false ]; then
          echo "✗ CRI not responding after 60 seconds"
          echo ""
          echo "=== Diagnostics ==="
          echo "Last containerd logs:"
          sudo tail -20 /var/log/kubernetes/containerd.log || echo "No logs"
          echo ""
          echo "Socket test:"
          ls -la /run/containerd/ || echo "Cannot list socket dir"
          echo ""
          echo "Process test:"
          ps aux | grep containerd | grep -v grep || echo "No containerd process"
          exit 1
        fi
        
        echo ""
        echo "=== Containerd info ==="
        crictl --runtime-endpoint unix:///run/containerd/containerd.sock info || echo "Cannot get containerd info"
        
        echo ""
        echo "✓ Containerd verification completed successfully"

    - name: Wait for stabilization
      run: |
        echo "=== Waiting for system stabilization ==="
        sleep 30
        
        # Проверяем что все процессы стабильны
        echo ""
        echo "=== Process check ==="
        ps aux | grep -E 'kube|etcd|containerd' | grep -v grep || true
        
        echo ""
        echo "=== Socket check ==="
        sudo netstat -tlnp | grep -E ':(6443|2379|10250)' || true

    # Критичный шаг: проверяем и исправляем kubeconfig
    - name: Fix kubeconfig if needed
      run: |
        # Добавляем kubectl в PATH
        export PATH=$PATH:./kubebuilder/bin
        
        echo "=== Checking kubeconfig in multiple locations ==="
        
        # Проверяем возможные пути
        KUBECONFIG_PATHS=(
          "$HOME/.kube/config"
          "/root/.kube/config"
          "/var/lib/kubernetes/kubeconfig"
          "/var/lib/kubelet/kubeconfig"
        )
        
        FOUND_CONFIG=""
        for config_path in "${KUBECONFIG_PATHS[@]}"; do
          if [ -f "$config_path" ]; then
            echo "✓ Found kubeconfig at: $config_path"
            ls -la "$config_path"
            FOUND_CONFIG="$config_path"
            
            echo ""
            echo "Content preview:"
            sudo head -20 "$config_path" || cat "$config_path" 2>/dev/null || true
            echo ""
          else
            echo "✗ Not found: $config_path"
          fi
        done
        
        if [ -z "$FOUND_CONFIG" ]; then
          echo ""
          echo "ERROR: No kubeconfig found in any expected location"
          echo "Checking if installer created it elsewhere:"
          sudo find /var/lib -name "config" -o -name "kubeconfig" 2>/dev/null || true
          exit 1
        fi
        
        echo ""
        echo "=== Using kubeconfig from: $FOUND_CONFIG ==="
        
        # Создаем .kube директорию в workspace
        mkdir -p ${{ github.workspace }}/.kube
        
        # Копируем найденный конфиг
        if [ "$FOUND_CONFIG" != "${{ github.workspace }}/.kube/config" ]; then
          echo "Copying kubeconfig to workspace"
          sudo cp "$FOUND_CONFIG" "${{ github.workspace }}/.kube/config"
          sudo chown $(id -u):$(id -g) "${{ github.workspace }}/.kube/config"
          chmod 600 "${{ github.workspace }}/.kube/config"
        fi
        
        echo ""
        echo "=== Parsing server URL ==="
        SERVER_URL=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}' 2>/dev/null || echo "")
        echo "Current server: '$SERVER_URL'"
        
        # Если kubeconfig пустой или неправильный - пересоздаём
        if [[ -z "$SERVER_URL" ]] || [[ "$SERVER_URL" != "https://127.0.0.1:6443" ]]; then
          echo ""
          echo "WARNING: kubeconfig is invalid or empty, recreating..."
          
          # Проверяем наличие сертификатов
          echo ""
          echo "=== Checking certificates ==="
          sudo ls -la /var/lib/kubernetes/pki/ || true
          
          if [ ! -f "/var/lib/kubernetes/pki/ca.crt" ]; then
            echo "ERROR: CA certificate not found"
            exit 1
          fi
          
          # Удаляем старый kubeconfig
          rm -f ${{ github.workspace }}/.kube/config
          
          # Настраиваем cluster
          kubectl config set-cluster local-cluster \
            --server=https://127.0.0.1:6443 \
            --certificate-authority=/var/lib/kubernetes/pki/ca.crt \
            --embed-certs=true
          
          # Настраиваем credentials
          if [ -f "/var/lib/kubernetes/pki/admin.crt" ] && [ -f "/var/lib/kubernetes/pki/admin.key" ]; then
            kubectl config set-credentials admin \
              --client-certificate=/var/lib/kubernetes/pki/admin.crt \
              --client-key=/var/lib/kubernetes/pki/admin.key \
              --embed-certs=true
          else
            echo "WARNING: Admin certificates not found, configuring without client auth"
            kubectl config set-credentials admin
          fi
          
          # Настраиваем context
          kubectl config set-context local-context \
            --cluster=local-cluster \
            --user=admin
          
          # Используем context
          kubectl config use-context local-context
          
          echo ""
          echo "=== Kubeconfig recreated ==="
        else
          echo ""
          echo "✓ kubeconfig is valid"
        fi
        
        echo ""
        echo "=== Final kubeconfig ==="
        kubectl config view --minify
        
        echo ""
        echo "=== Verify server URL ==="
        FINAL_SERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
        echo "Server: $FINAL_SERVER"
        
        if [[ "$FINAL_SERVER" != "https://127.0.0.1:6443" ]]; then
          echo "ERROR: Server URL is still wrong!"
          exit 1
        fi
        
        echo ""
        echo "✓ SUCCESS: kubeconfig correctly configured"

    - name: Test API TCP connection
      run: |
        echo "=== Testing TCP connection to API server ==="
        
        for i in {1..60}; do  # Увеличиваем количество попыток
          if timeout 5 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/6443' 2>/dev/null; then
            echo "✓ Connected to 127.0.0.1:6443 (attempt $i)"
            
            # Дополнительная проверка TLS
            if timeout 5 curl -k -s https://127.0.0.1:6443/healthz > /dev/null; then
              echo "✓ TLS handshake successful"
              exit 0
            else
              echo "⚠ TCP connected but TLS failed, retrying..."
            fi
          fi
          echo "Attempt $i/60 failed, retrying in 3 seconds..."
          sleep 3
        done
        
        echo "ERROR: Cannot connect to 127.0.0.1:6443 after 60 attempts"
        echo ""
        echo "=== Diagnostics ==="
        ps aux | grep kube-apiserver | grep -v grep || echo "API server not running"
        sudo netstat -tlnp | grep -E ':(6443|8080)' || echo "No listeners"
        sudo journalctl -u kube-apiserver --no-pager -n 20 || echo "No apiserver logs"
        exit 1

    - name: Test kubectl client
      run: |
        export PATH=$PATH:./kubebuilder/bin
        
        echo "=== kubectl client version ==="
        kubectl version --client
        
        echo ""
        echo "=== kubectl configuration ==="
        kubectl config current-context
        kubectl config get-contexts

    - name: Get cluster info
      run: |
        export PATH=$PATH:./kubebuilder/bin
        
        echo "=== Cluster info ==="
        if ! kubectl cluster-info --request-timeout=30s; then
          echo "Cannot get cluster info"
          echo ""
          echo "=== Config view ==="
          kubectl config view
          echo ""
          echo "=== API server health check ==="
          curl -k -v https://127.0.0.1:6443/healthz || echo "API not responding"
          echo ""
          echo "=== API server logs ==="
          sudo journalctl -u kube-apiserver --no-pager -n 30 || echo "No logs"
          exit 1
        fi

    - name: Verify cluster components
      run: |
        export PATH=$PATH:./kubebuilder/bin
        
        echo "=== Nodes ==="
        kubectl get nodes -o wide --request-timeout=30s || echo "Nodes check failed"
        
        echo ""
        echo "=== System pods ==="
        kubectl get pods -A --request-timeout=30s || echo "Pods check failed"
        
        echo ""
        echo "=== Component status ==="
        kubectl get componentstatuses --request-timeout=30s || echo "Status check failed"
        
        echo ""
        echo "=== API health ==="
        kubectl get --raw=/healthz --request-timeout=10s || echo "Health check failed"
        
        echo ""
        echo "=== Server version ==="
        kubectl version --request-timeout=30s || echo "Version check failed"
        
        echo ""
        echo "=== Namespaces ==="
        kubectl get namespaces --request-timeout=30s || echo "Namespace check failed"

    - name: Create test deployment
      run: |
        export PATH=$PATH:./kubebuilder/bin
        
        echo "=== Creating nginx deployment ==="
        if ! kubectl create deployment nginx --image=nginx:latest --request-timeout=30s; then
          echo "Deployment creation failed"
          kubectl get events --sort-by='.lastTimestamp' --request-timeout=30s
          exit 1
        fi
        
        echo ""
        echo "=== Waiting for deployment ==="
        if ! kubectl wait --for=condition=available --timeout=120s deployment/nginx; then
          echo "Deployment not ready within 120s"
          echo ""
          echo "=== Deployment status ==="
          kubectl get deployment nginx -o wide
          echo ""
          echo "=== Pod status ==="
          kubectl get pods -l app=nginx -o wide
          echo ""
          echo "=== Pod details ==="
          kubectl describe pods -l app=nginx
          echo ""
          echo "=== Events ==="
          kubectl get events --sort-by='.lastTimestamp'
          exit 1
        fi
        
        echo ""
        echo "=== Final deployment status ==="
        kubectl get deployments
        kubectl get pods -l app=nginx -o wide

    - name: Test service
      run: |
        export PATH=$PATH:./kubebuilder/bin
        
        echo "=== Creating service ==="
        kubectl expose deployment nginx --port=80 --type=NodePort --request-timeout=30s
        
        echo ""
        echo "=== Service details ==="
        kubectl get svc nginx -o wide
        kubectl describe svc nginx

    - name: Advanced diagnostics
      if: always()
      run: |
        echo "=== System info ==="
        uname -a
        echo ""
        echo "=== Disk usage ==="
        df -h
        echo ""
        echo "=== Memory usage ==="
        free -h
        echo ""
        echo "=== CPU info ==="
        nproc
        
        echo ""
        echo "=== Kubernetes processes ==="
        ps aux | grep -E 'kube|etcd|containerd' | grep -v grep || true
        
        echo ""
        echo "=== Network listeners ==="
        sudo netstat -tlnp | grep -E ':(6443|2379|10250|10251|10252)' || true
        
        echo ""
        echo "=== Containerd status ==="
        if command -v crictl > /dev/null; then
          export PATH=$PATH:./kubebuilder/bin
          crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a || true
          crictl --runtime-endpoint unix:///run/containerd/containerd.sock images || true
        fi
        
        echo ""
        echo "=== Directory structure ==="
        sudo ls -lah /var/lib/kubernetes/ || true
        sudo ls -lah /var/lib/kubernetes/pki/ || true
        sudo ls -lah /var/lib/kubelet/ || true
        sudo ls -lah /run/containerd/ || true

    - name: Collect comprehensive logs
      if: always()
      run: |
        mkdir -p k8s-logs
        
        # Системные журналы
        echo "=== Collecting systemd logs ==="
        sudo journalctl -u kube-apiserver --no-pager > k8s-logs/apiserver.log 2>&1 || echo "No apiserver logs" > k8s-logs/apiserver.log
        sudo journalctl -u kube-controller-manager --no-pager > k8s-logs/controller.log 2>&1 || echo "No controller logs" > k8s-logs/controller.log
        sudo journalctl -u kube-scheduler --no-pager > k8s-logs/scheduler.log 2>&1 || echo "No scheduler logs" > k8s-logs/scheduler.log
        sudo journalctl -u kubelet --no-pager > k8s-logs/kubelet.log 2>&1 || echo "No kubelet logs" > k8s-logs/kubelet.log
        sudo journalctl -u etcd --no-pager > k8s-logs/etcd.log 2>&1 || echo "No etcd logs" > k8s-logs/etcd.log
        sudo journalctl -u containerd --no-pager > k8s-logs/containerd-systemd.log 2>&1 || echo "No containerd systemd logs" > k8s-logs/containerd-systemd.log
        
        # Файловые логи
        echo "=== Collecting file logs ==="
        if [ -d "/var/log/kubernetes" ]; then
          sudo cp -r /var/log/kubernetes/* k8s-logs/ 2>&1 || true
        fi
        
        # Конфигурации
        echo "=== Collecting configurations ==="
        cp "${{ github.workspace }}/.kube/config" k8s-logs/kubeconfig.yaml 2>&1 || true
        sudo cp /etc/containerd/config.toml k8s-logs/containerd-config.toml 2>&1 || true
        
        # Kubernetes ресурсы
        echo "=== Collecting k8s resources ==="
        export PATH=$PATH:./kubebuilder/bin
        kubectl get all -A -o wide > k8s-logs/resources.txt 2>&1 || true
        kubectl get events -A --sort-by='.lastTimestamp' > k8s-logs/events.txt 2>&1 || true
        kubectl describe nodes > k8s-logs/nodes.txt 2>&1 || true
        kubectl get pods -A -o yaml > k8s-logs/pods-full.yaml 2>&1 || true
        
        # Системная информация
        echo "=== Collecting system info ==="
        ps aux > k8s-logs/processes.txt 2>&1 || true
        sudo netstat -tlnp > k8s-logs/network.txt 2>&1 || true
        df -h > k8s-logs/disk.txt 2>&1 || true
        free -h > k8s-logs/memory.txt 2>&1 || true
        
        # Installer лог
        cp installer.log k8s-logs/ 2>&1 || true
        
        echo "=== Log collection completed ==="
        ls -la k8s-logs/

    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: k8s-logs-${{ github.run_id }}
        path: k8s-logs/
        retention-days: 7

    - name: Upload binary
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: k8s-installer-${{ github.run_id }}
        path: k8s-installer
        retention-days: 30